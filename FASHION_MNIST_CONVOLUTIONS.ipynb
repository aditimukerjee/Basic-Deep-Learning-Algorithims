{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "FASHION MNIST CONVOLUTIONS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4LKVM2HEqWf"
      },
      "source": [
        "FASHION MNIST USING CONVOLUTIONS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYRzT-i2EqWp"
      },
      "source": [
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiZlkMZ9EqWq"
      },
      "source": [
        "# MY CALLBACKS\n",
        "\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('acc')>=0.998):\n",
        "      print(\"\\nReached 998% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYyhNo9gEqWr",
        "outputId": "2d637096-4601-43d8-aa2b-25e7c0166ad9"
      },
      "source": [
        "#importing the libary\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "#importing the mnist dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist     \n",
        "(training_images, training_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#training images and test images\n",
        "training_images=training_images.reshape(60000, 28, 28, 1) #reshape\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1) #reshape\n",
        "#You'll notice that there's a bit of a change here in that the training data needed to be reshaped.\n",
        "#That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 \n",
        "#items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images.\n",
        "test_images=test_images/255.0\n",
        "\n",
        "#my call backs\n",
        "callbacks = myCallback()\n",
        "#defining the models\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(256, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(256, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "#You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, \n",
        "#while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1.\n",
        "\n",
        "#compiling the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "#model summary\n",
        "model.summary()\n",
        "#model.summary() to see the size and shape of the network\n",
        "\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20, callbacks=[callbacks])\n",
        "\n",
        "print(\"------------------------------------------------------------------\")\n",
        "print(\"The performance of test dataset is as follows\")\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 256)       2560      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               11075840  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 11,080,970\n",
            "Trainable params: 11,080,970\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 294s 156ms/step - loss: 0.4492 - acc: 0.8409\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 291s 155ms/step - loss: 0.2341 - acc: 0.9129\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 292s 156ms/step - loss: 0.1767 - acc: 0.9341\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 293s 156ms/step - loss: 0.1342 - acc: 0.9492\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 294s 157ms/step - loss: 0.1019 - acc: 0.9617\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 292s 156ms/step - loss: 0.0739 - acc: 0.9737\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 292s 156ms/step - loss: 0.0606 - acc: 0.9780\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 292s 156ms/step - loss: 0.0441 - acc: 0.9844\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 297s 158ms/step - loss: 0.0378 - acc: 0.9860\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 295s 157ms/step - loss: 0.0276 - acc: 0.9910\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 294s 157ms/step - loss: 0.0271 - acc: 0.9913\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 292s 156ms/step - loss: 0.0218 - acc: 0.9926\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 293s 156ms/step - loss: 0.0190 - acc: 0.9938\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 293s 156ms/step - loss: 0.0205 - acc: 0.9925\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 293s 156ms/step - loss: 0.0145 - acc: 0.9951\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 295s 157ms/step - loss: 0.0130 - acc: 0.9957\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 295s 157ms/step - loss: 0.0155 - acc: 0.9949\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 295s 157ms/step - loss: 0.0129 - acc: 0.9956\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 294s 157ms/step - loss: 0.0106 - acc: 0.9965\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 293s 156ms/step - loss: 0.0091 - acc: 0.9972\n",
            "------------------------------------------------------------------\n",
            "The performance of test dataset is as follows\n",
            "313/313 [==============================] - 12s 36ms/step - loss: 0.7380 - acc: 0.9120\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTCUguLuNIDo"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keN_kr86EqWt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}